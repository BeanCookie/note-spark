#### 依赖血缘
RDD只支持粗粒度转换, 即在大量记录上执行的单个操作。将创建RDD的一系列Lineage(血统)记录下来, 以便恢复丢失的分区。RDD的 Lineage会记录RDD的元数据、信息和转换行为, 因为RDD本身不保存数据, 所以当该RDD的部分分区数据丢失时, 它可以根据这些信息来重新运算和恢复丢失的数据分区。

#### 为什么需要持久化
因为RDD再执行行动算子之前, 所有的转换算子都是懒加载的。只有在出发行动算子时依赖链上的转换算子才会真正的读取数据, 计算完成之后Spark会将内存中的数据清除。虽然可以节约内存, 但是每次Job都会从头执行一遍, 如何依赖链过长或者读取的原始数据过多, 那么整个过程就会很耗性能。

#### 持久化场景和策略
要持久化一个RDD只要调用其cache()或者persist()方法即可。在该RDD第一次被计算出来时就会直接缓存在每个节点中, 而且Spark的持久化机制是自动容错的, 如果持久化的RDD的任何pratition丢失了, Spark会自动通过其源RDD重新计算该parititon。

cache()和persist()的区别在于, cahe()是persist()的一种简化方式, cache()的底层就是调用的persist()的无参版本, 即persist(MEMORY_ONLY), 将数据持久化到内存中, 如果需要从内存中清除缓存, 调用unpersist()方法即可。Spark在进行shuffle操作的时候也会进行数据的持久化, 比如写入磁盘, 主要是为了避免节点失败时, 重新计算整个过程。

| 持久化级别                             | 含义描述                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| MEMORY_ONLY                            | 使用未序列化的Java对象格式, 将数据保存在内存中。如果内存不够存放所有的数据, 则数据可能就不会进行持久化, 默认的持久化策略 |
| MEMORY_AND_DISK                        | 使用未序列化的Java对象格式, 优先尝试将数据保存在内存中。如果内存不够存放所有的数据, 会将数据写入磁盘文件中不会立刻输出到磁盘 |
| MEMORY_ONLY_SER(Java and Scala)        | RDD的每个partition会被序列化成一个字节数组，节省空间，读取时间更占CPU |
| MEMORY_AND_DISK_SER(Java and Scala)    | 序列化存储，超出部分写入磁盘文件中                           |
| DISK_ONLY                              | 使用为序列化的Java对象格式, 将数据全部写入磁盘文件           |
| MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | 对于上述任意一种持久化策略, 如果后缀加上``_2``, 代表将每个持久化的数据都复制一份副本并保存到其他节点。 |
| OFF_HEAP (experimental)                | RDD序列化存储到Tachyon                                       |

